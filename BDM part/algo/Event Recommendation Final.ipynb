{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5cdbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, udf, monotonically_increasing_id\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de564a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 09:44:49 WARN Utils: Your hostname, wanglinhans-Laptop.local resolves to a loopback address: 127.0.0.1; using 192.168.1.141 instead (on interface en0)\n",
      "24/06/05 09:44:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/05 09:44:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/05 09:44:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Event Recommendation\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c70822c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+--------------------+--------------------+--------------+---------------------+--------------------+--------------------+\n",
      "|                 _id|               title|hosted_by|          event_time|          gmaps_link|vanue_location|vanue_location_detail|         description|              topics|\n",
      "+--------------------+--------------------+---------+--------------------+--------------------+--------------+---------------------+--------------------+--------------------+\n",
      "|{65f8d6b76c9cc986...|Coffee Walk & Bea...|  Mary G.|Friday, April 5, ...|https://www.googl...|    Itnig Caf√©| C. de Pujades, 10...|üóìÔ∏è FRI, APR 5 ‚Ä¢ ...|[Coffee, Social, ...|\n",
      "+--------------------+--------------------+---------+--------------------+--------------------+--------------+---------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.orc(\"data/meetup-20240407.orc\")\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd811ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=DoubleType())\n",
    "def extract_lat(gmaps_link):\n",
    "    try:\n",
    "        lat, lng = gmaps_link.split('query=')[1].split('%2C%20')\n",
    "        return float(lat)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# UDF to extract longitude from Google Maps link\n",
    "@udf(returnType=DoubleType())\n",
    "def extract_lon(gmaps_link):\n",
    "    try:\n",
    "        lat, lng = gmaps_link.split('query=')[1].split('%2C%20')\n",
    "        return float(lng)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def random_user_id():\n",
    "    import random\n",
    "    return 0 if random.random() < 0.5 else random.randint(1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1384f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"lat\", extract_lat(col(\"gmaps_link\")))\n",
    "df = df.withColumn(\"lon\", extract_lon(col(\"gmaps_link\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86fa0098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"post_id\", monotonically_increasing_id() + 1) \\\n",
    "       .withColumn(\"user_id\", random_user_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37888f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\n",
    "    col(\"post_id\"),\n",
    "    col(\"user_id\"),\n",
    "    col(\"title\"),\n",
    "    col(\"description\"),\n",
    "    col(\"topics\"),\n",
    "    col(\"event_time\"),\n",
    "    col(\"lon\"),\n",
    "    col(\"lat\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af3190f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------+--------------------------------------------+--------+--------+\n",
      "|post_id|user_id|title                                                 |description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |topics                                                  |event_time                                  |lon     |lat     |\n",
      "+-------+-------+------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------+--------------------------------------------+--------+--------+\n",
      "|1      |0      |Coffee Walk & Beach Sunset Hangout to Make New Friends|üóìÔ∏è FRI, APR 5 ‚Ä¢ 6:30 PM CET ‚òïÔ∏èüåÖ Coffee Walk & Beach Sunset Hangout to Make New Friends! Ladies of Barcelona, ages 18-29, you're invited to join us to get coffee to-go at Itnig Cafe and then walk to the beach to enjoy the sunset and some snacks! üë≠ There will be a max of 10 people, so please don‚Äôt hesitate to come alone. The purpose of this event is to meet new friends! üíõWe believe the best way to foster intentional conversation is by being fully present with the people you‚Äôre with. üìµTherefore, we kindly ask everyone to turn their phones off during the event. If you want to take photos, there will be a camera for pictures, so don‚Äôt worry!|[Coffee, Social, Make New Friends, Picnics, Cafe Lovers]|Friday, April 5, 20246:30 PM to 8:30 PM CEST|2.194162|41.39655|\n",
      "+-------+-------+------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------+--------------------------------------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(1).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28f3614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.json(\"data/posts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d8229",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed945105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wanglinhan/miniconda3/envs/joint/lib/python3.9/site-packages/nltk/metrics/association.py:26: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 1.21.3)\n",
      "  from scipy.stats import fisher_exact\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, row_number, explode\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05d38a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/wanglinhan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/wanglinhan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/wanglinhan/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wanglinhan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "VERB_CODES = {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08b9361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(text):\n",
    "    text = text.lower()\n",
    "    temp_sent = []\n",
    "    words = word_tokenize(text)\n",
    "    tags = pos_tag(words)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if tags[i][1] in VERB_CODES:\n",
    "            lemmatized = lemmatizer.lemmatize(word, 'v')\n",
    "        else:\n",
    "            lemmatized = lemmatizer.lemmatize(word)\n",
    "        if lemmatized not in stop_words and lemmatized.isalpha():\n",
    "            temp_sent.append(lemmatized)\n",
    "    \n",
    "    finalsent = ' '.join(temp_sent)\n",
    "    finalsent = finalsent.replace(\"n't\", \" not\")\n",
    "    finalsent = finalsent.replace(\"'m\", \" am\")\n",
    "    finalsent = finalsent.replace(\"'s\", \" is\")\n",
    "    finalsent = finalsent.replace(\"'re\", \" are\")\n",
    "    finalsent = finalsent.replace(\"'ll\", \" will\")\n",
    "    finalsent = finalsent.replace(\"'ve\", \" have\")\n",
    "    finalsent = finalsent.replace(\"'d\", \" would\")\n",
    "    \n",
    "    return finalsent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a926f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_udf = udf(preprocess_sentences, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c84e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"description_proc\", preprocess_udf(df[\"description\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c0ce19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"description_proc\", outputCol=\"description_proc_tokens\")\n",
    "df = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfdb5327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wanglinhan/miniconda3/envs/joint/lib/python3.9/site-packages/nltk/metrics/association.py:26: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 1.21.3)\n",
      "  from scipy.stats import fisher_exact\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"description_proc_tokens\", outputCol=\"features\")\n",
    "cv_model = cv.fit(df)\n",
    "df = cv_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7de15859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_dense(sparse_vector):\n",
    "    return Vectors.dense(sparse_vector.toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7213c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_to_dense_udf = udf(sparse_to_dense, VectorUDT())\n",
    "df = df.withColumn(\"dense_features\", sparse_to_dense_udf(df[\"features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72f4e526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.1/libexec/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py:81: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 1.21.3)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dense_features = df.select(\"dense_features\").collect()\n",
    "dense_feature_array = np.array([row[\"dense_features\"].toArray() for row in dense_features])\n",
    "cosine_sim = cosine_similarity(dense_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e319269",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_broadcast = spark.sparkContext.broadcast(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b3b8b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_id_mapping = {row[\"post_id\"]: idx for idx, row in enumerate(df.select(\"post_id\").collect())}\n",
    "post_id_mapping_broadcast = spark.sparkContext.broadcast(post_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2e8f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"post_id\").desc())\n",
    "df = df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "last_posts_df = df.filter(col(\"rank\") == 1).select(\"post_id\", \"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65a19fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(post_id, num_recommendations=5):\n",
    "    post_idx = post_id_mapping_broadcast.value.get(post_id, None)\n",
    "    if post_idx is None:\n",
    "        return []\n",
    "    \n",
    "    sim_scores = cosine_sim_broadcast.value[post_idx]\n",
    "    sim_scores = list(enumerate(sim_scores))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = [post_id_mapping_broadcast.value.get(idx, None) for idx, score in sim_scores if idx != post_idx][:num_recommendations]\n",
    "    return [pid for pid in sim_scores if pid is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbccc93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations_udf = udf(lambda post_id: get_recommendations(post_id), ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44f714f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_posts_df = last_posts_df.withColumn(\"recommendations\", get_recommendations_udf(col(\"post_id\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2928f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df = last_posts_df.select(col(\"user_id\"), col(\"post_id\").alias(\"last_post_id\"), col(\"recommendations\"))\n",
    "recommendations_df = recommendations_df.withColumn(\"recommendation_post_id\", explode(col(\"recommendations\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd5a1e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df = recommendations_df.withColumn(\"recommendation_id\", row_number().over(Window.orderBy(monotonically_increasing_id())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cf6827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = recommendations_df.select(col(\"recommendation_id\"), col(\"user_id\"), col(\"recommendation_post_id\").alias(\"post_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8d26266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 09:48:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/05 09:48:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/05 09:48:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/05 09:48:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/05 09:48:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/05 09:48:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+-------+\n",
      "|recommendation_id|user_id|post_id|\n",
      "+-----------------+-------+-------+\n",
      "|1                |0      |51     |\n",
      "|2                |0      |20     |\n",
      "|3                |0      |45     |\n",
      "|4                |0      |65     |\n",
      "|5                |0      |35     |\n",
      "|6                |1      |142    |\n",
      "|7                |1      |143    |\n",
      "|8                |1      |144    |\n",
      "|9                |1      |145    |\n",
      "|10               |1      |102    |\n",
      "+-----------------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b091917a",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a4bb220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|post_id|user_id|title                                                           |description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+-------+-------+----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|102    |19     |Yoga Class on Sailboat                                          |Join us for a magical evening of Yoga and Meditation while cruising along the stunning Barcelona coastline. This unique experience combines the serenity of the sea with the transformative power of self-empowerment and spiritual growth. Whether you're a seasoned yogi or just starting your wellness journey, this event is designed to help you connect with your inner self, increase mindfulness, and find balance in both body and mind. Come aboard to cultivate a sense of mindfulness, enhance your spirituality, and embrace a healthier lifestyle while surrounded by the tranquil waters of the Mediterranean.                                                                                                                                                                                                                                                                      |\n",
      "|141    |0      |Tech Synergy: Mastering Integration and Performance Optimization|üåüJoin Us for an Exciting Tech Talk MeetUp Next Tuesday! üåü We're thrilled to invite you to our upcoming Tech Talk MeetUp where we'll delve into the world of Backend Software Engineering. Here's what's on the agenda: üîç Gokul will be taking us on a deep dive into JVM & GC, shedding light on how we managed to slash our tail latencies by a whopping 90%! üöÄ üí° Igor will be sharing insights on bridging the gap between API Integration and overcoming common obstacles in the process. Don't miss out on this opportunity to expand your knowledge and gain valuable insights into these critical topics. Looking forward to seeing you all there! üöÄ Date: 19th March, 6:30pm Glovo HQ: C. Llull, 110, Sea Side Amphitheatre Featuring speakers : Gokul Srinivas (Software Engineer IV- Backend) and Igor Sakhankov (Software Engineer IV - Backend) Register now and secure your spot!|\n",
      "|143    |11     |Yoga + Connections by the beach                                 |~ WHAT? 45min yoga + 45min Personal development discussions‚Ä®‚Ä®~ WHEN? Every Sunday 10am‚Ä®‚Ä®~ WHERE? Platja del F√≥rum, Barcelona‚Ä®https://maps.app.goo.gl/djRmNfCSnXfhgudq7?g_st=ic‚Ä®‚Ä®~ HOW MUCH? Free - If you want you can donate by Cash or Bizum +34 623 119 893‚Ä®‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Ä©Hey I am Zsofia. I am passionate about yoga and personal development so I want to blend in the two and create a community, a safe space, where you can share your thoughts and connect with one another on a deeper level.‚Ä©- You can join for both parts or just one of the activities as you wish.‚Ä©- Please bring a mat, warm clothes, and maybe a blanket for the chatting part JOIN WHATSAPP FOR UPDATES https://chat.whatsapp.com/EIqE8YYKMRY5T9NdQ96A7W                                                                                                                                                 |\n",
      "|144    |0      |Yoga + Connections by the beach                                 |~ WHAT? 45min yoga + 45min Personal development discussions‚Ä®‚Ä®~ WHEN? Every Sunday 10am‚Ä®‚Ä®~ WHERE? Platja del F√≥rum, Barcelona‚Ä®https://maps.app.goo.gl/djRmNfCSnXfhgudq7?g_st=ic‚Ä®‚Ä®~ HOW MUCH? Free - If you want you can donate by Cash or Bizum +34 623 119 893‚Ä®‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Ä©Hey I am Zsofia. I am passionate about yoga and personal development so I want to blend in the two and create a community, a safe space, where you can share your thoughts and connect with one another on a deeper level.‚Ä©- You can join for both parts or just one of the activities as you wish.‚Ä©- Please bring a mat, warm clothes, and maybe a blanket for the chatting part JOIN WHATSAPP FOR UPDATES https://chat.whatsapp.com/EIqE8YYKMRY5T9NdQ96A7W                                                                                                                                                 |\n",
      "|145    |10     |Yoga + Connections by the beach                                 |~ WHAT? 45min yoga + 45min Personal development discussions‚Ä®‚Ä®~ WHEN? Every Sunday 10am‚Ä®‚Ä®~ WHERE? Platja del F√≥rum, Barcelona‚Ä®https://maps.app.goo.gl/djRmNfCSnXfhgudq7?g_st=ic‚Ä®‚Ä®~ HOW MUCH? Free - If you want you can donate by Cash or Bizum +34 623 119 893‚Ä®‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Ä©Hey I am Zsofia. I am passionate about yoga and personal development so I want to blend in the two and create a community, a safe space, where you can share your thoughts and connect with one another on a deeper level.‚Ä©- You can join for both parts or just one of the activities as you wish.‚Ä©- Please bring a mat, warm clothes, and maybe a blanket for the chatting part JOIN WHATSAPP FOR UPDATES https://chat.whatsapp.com/EIqE8YYKMRY5T9NdQ96A7W                                                                                                                                                 |\n",
      "+-------+-------+----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "post_ids_to_filter = [141, 143, 144, 145, 102] \n",
    "\n",
    "filtered_df = df.filter(df.post_id.isin(post_ids_to_filter)).select(\"post_id\", \"user_id\", \"title\", \"description\")\n",
    "filtered_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b258355a",
   "metadata": {},
   "source": [
    "# Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45551a6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/04 23:39:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/04 23:39:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/04 23:39:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/04 23:39:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/04 23:39:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/04 23:39:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/04 23:39:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "final_df.write.csv(\"data/recommendations.csv\", header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
